<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Detecting Visual Relationships with Deep Relational Networks（阅读笔记） | Wenyuan</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</h1><a id="logo" href="/.">Wenyuan</a><p class="description">Wenyuan Xue's Blog</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</h1><div class="post-meta">Jul 29, 2017<span> | </span><span class="category"><a href="/categories/文献阅读笔记/">文献阅读笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a href="/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/#comments" class="ds-thread-count cloud-tie-join-count"><span style="font-size: 15px; color: #6E7173;" class="join-count">0</span><span> 条参与</span></a><div class="post-content"><p><strong>This is an original article. Please indicate reference if reproduced.</strong>  </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>图像中的真实世界通常存在多种相互作用的物体。为了理解这些图像，仅仅做到对单个物体的识别是远远不够的。物体间的关系也存在着很重要的信息。例如，图像说明（image caption）是一种很受欢迎的计算机视觉应用能够基于图像中物体的关系生成丰富的语言描述。随着深度学习的快速发展，我们目睹了计算机视觉在物体识别、场景分类、抽象检测等几项关键任务中取得的显著成就。然而，视觉关系检测仍然是非常困难的一项任务。Visual Genome，一个用于结构性图像理解的大型数据集，最好的方法在Recall@50指标中仅仅能达到11.9%的结果。显然这个结果远远不能让人满意。</p>
<p>任务描述：<br>输入是一张含有多个物体的图像，对于图像中的一对物体，输出是一个三元组（s，r，o），分别表示这对物体的主语（物体1）、谓语（关系）和宾语（物体2）。如下图所示：<br><img src="https://user-images.githubusercontent.com/7368805/28743149-65c97f38-7475-11e7-92e2-66463c031e6e.png" alt="图1:视觉关系广泛存在与真实世界图像中。这里是一些来自于VRD的样例图，他们拥有关系连接词“sit”和“carry”，我们开发了一种方法可以有效的从给定的图像中检测出这样子的视觉关系，如上图所示，场景图也可以被构造出来"></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>相关工作主要有以下几项内容：  </p>
<ul>
<li>早期的工作主要聚焦在特殊类型的关系上（如位置、动作），主要用来帮助完成其他任务（如目标检测、图像分类与检索等等）。</li>
<li>然后，有一种思路是把每一种可区分的物体组合与关系词共同作为一类（也称视觉短语）。但是这种方法很难应对一般的情况，因为这种组合的数量非常庞大。</li>
<li>还有一种思路是将关系词与物体类别分别预测。</li>
<li>Lu et al. 在2016的工作是与我们最相关的。他们将一对检测到的物体送到分类器中，并结合了表层特征和语言先验用来识别物体间的关系。</li>
</ul>
<h2 id="Visual-Relationship-Detection"><a href="#Visual-Relationship-Detection" class="headerlink" title="Visual Relationship Detection"></a>Visual Relationship Detection</h2><p><img src="https://user-images.githubusercontent.com/7368805/28743269-8d4039c8-7478-11e7-9621-314afa6372e4.png" alt="图2:视觉关系检测流程图"></p>
<ol>
<li><strong>目标检测</strong>  给一张图像作为输入，我们首先使用Faster RCNN进行目标检测，每个检测到的候选区域都有一个边框信息和表层特征。      </li>
<li><strong>物体对过滤</strong> 对于n个检测到的物体，一共可以组合成$n\multi(n-1)$对物体，图像中的物体不可能都具有关系，如一些距离较远的物体就不太可能存在相互联系。因此，作者设计了一种低复杂度的神经网络用来对这些物体对进行筛选。</li>
<li><strong>联合识别</strong> 该模块结合了多种特征作为输入，并输出一个三元组（s，r，o）作为输出：<ul>
<li><strong>表征（Appearance）</strong> 除了可以从上述目标检测过程中获取单个物体的表征之外，物体间的关系也反应在两个物体共同存在的视觉区域上。因此，将两个物体共同存在的区域框出来，并适当拓展边缘，通过Appr Module抽取特征，作为联合识别模块的一组特征。值得注意的是，从图2中可以看出，主语（物体1）和宾语（物体2）的特征分别传递到了每次DR-Net的迭代过程中。</li>
<li><strong>空间结构（Spatial Configurations）</strong> 两物体的关系也可以反应在空间结构上。这种特性可以充分表示单个物体，也对光照变化具有容错性。之前的工作通过几何度量来表示空间结构，本文使用来双空间模板来进行表示，他们分别由两个二值模板构成，一个表示主语，一个表示宾语。每个模板都被下采样为32x32大小。每个物体对的双空间模板将通过三层卷积网络被压缩成为一个64维的向量。</li>
<li><strong>统计关系（Statistical Relations）</strong> 在一个三元组（s，r，o）中，关系词与主语和宾语通常存在着很强的统计依赖性。例如，（cat，eat，fish）是正常的，但是（fish，eat，cat）或者（cat，ride，fish）却不太可能。在Visual Genome数据集中，如果关系词先验分布的$p(r)$是2.88，则条件概率$p(r|s, o)$是1.21。这个差别也清楚得说明两统计依赖性。<br>我们因此提出了DR-Net，使用深度神经网络来表示这种统计依赖性。在实验中，作者发现这种关系可以有效解决由于视觉或者空间特征引起的奇异</li>
<li><strong>综合判断（Integrated Prediction）</strong> 如图2所示，对于每一对物体，分别通过表征模块和空间模块来抽取表层特征和空间特征。然后，把两种特征连结起来通过两层全连结网络进行压缩。被压缩过的网络继而传递到DR-Net进行联合推理。通过多个推理单元，DR-Net网络将输出s，r，o的后验概率。最终，整个框架将选择最有可能的结果作为输出。<br>在训练中，框架中的三个阶段，object detection, pair filtering以及joint recognition被分别训练。至于joint recognition阶段，不同的特征综合到一起作为输入传给一个网络，然后联合微调来最大化联合概率。</li>
</ul>
</li>
</ol>
<h2 id="Deep-Relational-Network"><a href="#Deep-Relational-Network" class="headerlink" title="Deep Relational Network"></a>Deep Relational Network</h2><p>对于视觉关系识别任务，条件随机场（CRF）可以表示为：</p>
<p>$$<br>p(r, s ,o|X_r, X_s, X_o)=\frac{1}{Z}exp(\Phi(r, s ,o|X_r, X_s, X_o; W))<br>$$</p>
<p>其中，$X_r$表示压缩后的物体对的特征，$X_s$和$X_o$分别表示主语和宾语的特征。$W$表示模型参数，$Z$表示正则化约束。联合隐变量$\Phi$可以表示为多个单隐变量的和：</p>
<p>$$<br>\Phi=\phi_a(s|X_s;W_a)+\phi_a(o|X_o;W_a)+\phi_r(r|X_r;W<em>r)+\phi</em>{rs}(r,s|W<em>{rs})+\phi</em>{ro}(r,o|W<em>{ro})+\phi</em>{so}(s,o|W_{so})<br>$$</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2></div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://xuewenyuan.github.io/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/" data-id="cj5p6rtx20005g301a2omzihv" class="article-share-link">分享</a><div class="tags"><a href="/tags/目标检测/">目标检测</a><a href="/tags/视觉关系/">视觉关系</a><a href="/tags/深度相关网络/">深度相关网络</a></div><div class="post-nav"><a href="/2017/06/30/Deep-Forest-Towards-an-Alternative-to-Deep-Neural-Networks-阅读笔记/" class="next">Deep Forest: Towards an Alternative to Deep Neural Networks (阅读笔记)</a></div><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script>var cloudTieConfig = {
  url: document.location.href,
  productKey: "true",
  target: "cloud-tie-wrapper"
};</script><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://xuewenyuan.github.io"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Neural-Networks/">Neural Networks</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/文献阅读笔记/">文献阅读笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Gabor-Filter/" style="font-size: 15px;">Gabor Filter</a> <a href="/tags/深度森林/" style="font-size: 15px;">深度森林</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Object-Detection/" style="font-size: 15px;">Object Detection</a> <a href="/tags/R-CNN/" style="font-size: 15px;">R-CNN</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/视觉关系/" style="font-size: 15px;">视觉关系</a> <a href="/tags/深度相关网络/" style="font-size: 15px;">深度相关网络</a> <a href="/tags/gcForest/" style="font-size: 15px;">gcForest</a> <a href="/tags/Neural-Networks/" style="font-size: 15px;">Neural Networks</a> <a href="/tags/Perceptron/" style="font-size: 15px;">Perceptron</a> <a href="/tags/Multilayer-Perceptron/" style="font-size: 15px;">Multilayer Perceptron</a> <a href="/tags/BackPropagation/" style="font-size: 15px;">BackPropagation</a> <a href="/tags/文本分割/" style="font-size: 15px;">文本分割</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/CTC/" style="font-size: 15px;">CTC</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/Deep-Forest-Towards-an-Alternative-to-Deep-Neural-Networks-阅读笔记/">Deep Forest: Towards an Alternative to Deep Neural Networks (阅读笔记)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/21/DeepLearning4ObjectDetection/">Deep Learning for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/20/TexLive+Atom/">TexLive + Atom,教你配置炫酷的LaTex编译环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/19/Paragraph-text-segmentation-into-lines-with-Recurrent-Neural-Networks-阅读笔记/">Paragraph text segmentation into lines with Recurrent Neural  Networks（阅读笔记）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/14/Neural-Networks-1/">Neural Networks(Part 1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/27/How-To-Understand-Gabor-Filter/">如何理解Gabor滤波器</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/24/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://yuhaomin.github.io/" title="俞浩敏" target="_blank">俞浩敏</a><ul></ul><a href="http://blog.csdn.net/jzwong" title="王建柱" target="_blank">王建柱</a><ul></ul><a href="http://blog.csdn.net/gyarenas" title="耿阳李敖" target="_blank">耿阳李敖</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">Wenyuan.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>