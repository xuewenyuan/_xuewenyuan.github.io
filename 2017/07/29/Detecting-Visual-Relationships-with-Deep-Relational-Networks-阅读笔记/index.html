<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Detecting Visual Relationships with Deep Relational Networks（阅读笔记） | Wenyuan</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</h1><a id="logo" href="/.">Wenyuan</a><p class="description">Wenyuan Xue's Blog</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</h1><div class="post-meta">Jul 29, 2017<span> | </span><span class="category"><a href="/categories/文献阅读笔记/">文献阅读笔记</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span></div><a href="/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/#comments" class="ds-thread-count cloud-tie-join-count"><span style="font-size: 15px; color: #6E7173;" class="join-count">0</span><span> 条参与</span></a><div class="post-content"><h2 id=""><a href="#" class="headerlink" title=""></a><a id="more"></a></h2><p><strong>This is an original article. Please indicate reference if reproduced.</strong>  </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>图像中的真实世界通常存在多种相互作用的物体。为了理解这些图像，仅仅做到对单个物体的识别是远远不够的。物体间的关系也存在着很重要的信息。例如，图像说明（image caption）是一种很受欢迎的计算机视觉应用能够基于图像中物体的关系生成丰富的语言描述。随着深度学习的快速发展，我们目睹了计算机视觉在物体识别、场景分类、抽象检测等几项关键任务中取得的显著成就。然而，视觉关系检测仍然是非常困难的一项任务。Visual Genome，一个用于结构性图像理解的大型数据集，最好的方法在Recall@50指标中仅仅能达到11.9%的结果。显然这个结果远远不能让人满意。</p>
<p>任务描述：<br>输入是一张含有多个物体的图像，对于图像中的一对物体，输出是一个三元组（s，r，o），分别表示这对物体的主语（物体1）、谓语（关系）和宾语（物体2）。如下图所示：<br><img src="https://user-images.githubusercontent.com/7368805/28743149-65c97f38-7475-11e7-92e2-66463c031e6e.png" alt="图1:视觉关系广泛存在与真实世界图像中。这里是一些来自于VRD的样例图，他们拥有关系连接词“sit”和“carry”，我们开发了一种方法可以有效的从给定的图像中检测出这样子的视觉关系，如上图所示，场景图也可以被构造出来"></p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>相关工作主要有以下几项内容：  </p>
<ul>
<li>早期的工作主要聚焦在特殊类型的关系上（如位置、动作），主要用来帮助完成其他任务（如目标检测、图像分类与检索等等）。</li>
<li>然后，有一种思路是把每一种可区分的物体组合与关系词共同作为一类（也称视觉短语）。但是这种方法很难应对一般的情况，因为这种组合的数量非常庞大。</li>
<li>还有一种思路是将关系词与物体类别分别预测。</li>
<li>Lu et al. 在2016的工作是与我们最相关的。他们将一对检测到的物体送到分类器中，并结合了表层特征和语言先验用来识别物体间的关系。</li>
</ul>
<h2 id="Visual-Relationship-Detection"><a href="#Visual-Relationship-Detection" class="headerlink" title="Visual Relationship Detection"></a>Visual Relationship Detection</h2><p><img src="https://user-images.githubusercontent.com/7368805/28743269-8d4039c8-7478-11e7-9621-314afa6372e4.png" alt="图2:视觉关系检测流程图"></p>
<ol>
<li><strong>目标检测</strong>  给一张图像作为输入，我们首先使用Faster RCNN进行目标检测，每个检测到的候选区域都有一个边框信息和表层特征。      </li>
<li><strong>物体对过滤</strong> 对于n个检测到的物体，一共可以组合成$n\multi(n-1)$对物体，图像中的物体不可能都具有关系，如一些距离较远的物体就不太可能存在相互联系。因此，作者设计了一种低复杂度的神经网络用来对这些物体对进行筛选。</li>
<li><strong>联合识别</strong> 该模块结合了多种特征作为输入，并输出一个三元组（s，r，o）作为输出：<ul>
<li><strong>表征（Appearance）</strong> 除了可以从上述目标检测过程中获取单个物体的表征之外，物体间的关系也反应在两个物体共同存在的视觉区域上。因此，将两个物体共同存在的区域框出来，并适当拓展边缘，通过Appr Module抽取特征，作为联合识别模块的一组特征。值得注意的是，从图2中可以看出，主语（物体1）和宾语（物体2）的特征分别传递到了每次DR-Net的迭代过程中。</li>
<li><strong>空间结构（Spatial Configurations）</strong> 两物体的关系也可以反应在空间结构上。这种特性可以充分表示单个物体，也对光照变化具有容错性。之前的工作通过几何度量来表示空间结构，本文使用来双空间模板来进行表示，他们分别由两个二值模板构成，一个表示主语，一个表示宾语。每个模板都被下采样为32x32大小。每个物体对的双空间模板将通过三层卷积网络被压缩成为一个64维的向量。</li>
<li><strong>统计关系（Statistical Relations）</strong> 在一个三元组（s，r，o）中，关系词与主语和宾语通常存在着很强的统计依赖性。例如，（cat，eat，fish）是正常的，但是（fish，eat，cat）或者（cat，ride，fish）却不太可能。在Visual Genome数据集中，如果关系词先验分布的$p(r)$是2.88，则条件概率$p(r|s, o)$是1.21。这个差别也清楚得说明两统计依赖性。<br>我们因此提出了DR-Net，使用深度神经网络来表示这种统计依赖性。在实验中，作者发现这种关系可以有效解决由于视觉或者空间特征引起的奇异</li>
<li><strong>综合判断（Integrated Prediction）</strong> 如图2所示，对于每一对物体，分别通过表征模块和空间模块来抽取表层特征和空间特征。然后，把两种特征连结起来通过两层全连结网络进行压缩。被压缩过的网络继而传递到DR-Net进行联合推理。通过多个推理单元，DR-Net网络将输出s，r，o的后验概率。最终，整个框架将选择最有可能的结果作为输出。<br>在训练中，框架中的三个阶段，object detection, pair filtering以及joint recognition被分别训练。至于joint recognition阶段，不同的特征综合到一起作为输入传给一个网络，然后联合微调来最大化联合概率。</li>
</ul>
</li>
</ol>
<h2 id="Deep-Relational-Network"><a href="#Deep-Relational-Network" class="headerlink" title="Deep Relational Network"></a>Deep Relational Network</h2><p>对于视觉关系识别任务，条件随机场（CRF）可以表示为：</p>
<p>$$<br>p(r, s ,o|X_r, X_s, X_o)=\frac{1}{Z}exp(\Phi(r, s ,o|X_r, X_s, X_o; W))<br>$$</p>
<p>其中，$X_r$表示压缩后的物体对的特征，$X_s$和$X_o$分别表示主语和宾语的特征。$W$表示模型参数，$Z$表示正则化约束。联合隐变量$\Phi$可以表示为多个单隐变量的和：</p>
<p>$$<br>\Phi={\psi}_a(s|X_s;W_a)+{\psi}_a(o|X_o;W_a)+{\psi}_r(r|X_r;W_r)  \\<br>+{\varphi}_{rs}(r, s|W_{rs})+{\varphi}_{ro}(r,o|W_{ro})+{\varphi}_{so}(s,o|W_{so})<br>$$</p>
<p>其中，单隐变量${\psi}_a$联系主语（宾语）与起表征；${\psi}_r$联系关系词与特征$X_r$；二元潜变量${\varphi}_{rs}$，${\varphi}_{ro}$和${\varphi}_{so}$用来表示关系词r，主语类别s，和宾语类别o之间的关系。</p>
<p>基于上述CRF的表示，给定s和o，则r的后验概率可以表示为：</p>
<p>$$<br>p(r|s,o,X_r;W){\propto}exp({\psi}_r(r|X_r;W_r)  \\<br>+{\varphi}_{rs}(r, s|W_{rs})+{\varphi}_{ro}(r,o|W_{ro}))<br>$$</p>
<p>一般情况下，${\psi}_r(r|X_r;W_r)$通常被设计为线性函数。让$W_{rs}$和$W_{ro}$分别为矩阵，则有$W_{rs}(r,s)={\varphi}_{rs}(r,s)$和$W_{ro}(r,o)={\varphi}_{ro}(r,o)$。并且，令$q_r$表示为r的后验概率向量，然后上述公式可以重新写作：</p>
<p>$$<br>q_r={\sigma}(W_rX_r+W_{rs}1_s+W_{ro}1_o)<br>$$</p>
<p>这里，$\sigma$表示softmax函数。$1_s$和$1_s$分别表示s和r的one-hot指示向量。因此整个问题如下的优化问题：</p>
<p>$$<br>\max_{q}E_q[{\psi}_r(r|X_r;W_r) \\<br>+{\varphi}_{rs}(r, s|W_{rs})+{\varphi}_{ro}(r,o|W_{ro})]+H_q(q)<br>$$</p>
<p>如果s和o是不确定的，并且以概率向量$q_s$和$q_o$来表示，则：</p>
<p>$$<br>q_r={\sigma}(W_rX_r+W_{rs}q_s+W_{ro}q_o)<br>$$</p>
<p>同理，这个推理公式可以用到s和o上来，因此，我们就可以得到如下更新公式：</p>
<p>$$<br>q^{‘}_s={\sigma}(W_aX_s+W_{sr}q_r+W_{so}q_o) \\<br>q^{‘}_r={\sigma}(W_rX_r+W_{rs}q_s+W_{ro}q_o) \\<br>q^{‘}_o={\sigma}(W_aX_o+W_{os}q_s+W_{or}q_r)<br>$$</p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>数据集</strong><br>VRD：5000张图像，37993种视觉关系，6672种三元组<br>sVG：108k张图像，998k种视觉关系，74361种三元组</p>
<p><strong>实验配置</strong><br>模型训练.所有实验都使用来Caffe架构。表征模块使用ImageNet来预训练，空间模块和DR-Net随机初始化。初始化之后，整个网络使用SGD进行联合优化。  </p>
<p>性能衡量.Recall@50和Recall@100。分别根据概率取前50和100个结果计算召回率。  </p>
<p>任务设定.1）Predicate recognition，在给定主语和宾语标签以及坐标的情况下判断其关系词。2）Union box detection，将整个三元组所在区域用bounding box框出来，要求准确预测出三元组（s，r，o）以及bounding box的坐标，Iou与ground-truth的重叠率需在50%以上。3）Two boxes detection，与2）相似，要求分别预测主语与宾语的bounding box位置。  </p>
<p><strong>结果比较</strong><br>文中进行了三组对照实验。  </p>
<p>与baselines的比较：<br>本文方法虽然在关系识别中取得了很好的性能，但是在Union box detection和Two boxes detection两项指标下的结果却并没有得到显著提高。作者初步分析是由于目标检测器造成的。实验发现目标检测的Recall@50大约只有30%。  </p>
<p>在该方法的不同配置下进行比较，得到如下结论：</p>
<ol>
<li>使用更好的特征提取网络结构可以取得更好的结果。（ResNet101 vs. VGG16）</li>
<li>表征和空间特征结合起来比只使用其中一种取得的效果要好。</li>
<li>物体间关系的统计依赖是非常重要的，实验表明其可以帮助解决关系词的歧义问题。本文中，统计依赖的学习通过DR-Net来完成。</li>
</ol>
<p>比较不同结构下的性能：<br>主要有两方面结论</p>
<ol>
<li>随着推理单元的增加，Recall@1的结果也在上升。</li>
<li>推理单元的参数不进行共享比共享下的性能要好，因为可以复杂的参数有更好的表达能力。</li>
</ol>
<p><strong>场景图生成</strong><br>该模型也可用于场景图生成。生成的场景图可应用与图像描述、视觉问答和图像检索等基本任务中。<br><img src="https://user-images.githubusercontent.com/7368805/28745614-586e9ea2-74ae-11e7-8439-9abece366316.png" alt=""></p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>与图像的caption任务相比，研究图像中物体间的关系这个任务更加具体。而且因为数据集和相关研究相对较少，有在不同场景中继续发觉研究价值的前景。</p>
<hr>
<p><strong>如有任何相关问题欢迎评论或者邮件讨论<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#x31;&#x35;&#x31;&#x32;&#x30;&#x34;&#x35;&#x32;&#64;&#x62;&#x6a;&#116;&#117;&#x2e;&#101;&#x64;&#117;&#x2e;&#x63;&#x6e;">&#x31;&#x35;&#x31;&#x32;&#x30;&#x34;&#x35;&#x32;&#64;&#x62;&#x6a;&#116;&#117;&#x2e;&#101;&#x64;&#117;&#x2e;&#x63;&#x6e;</a></strong></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://xuewenyuan.github.io/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/" data-id="cj5pei8j40002xy0132070scv" class="article-share-link">分享</a><div class="tags"><a href="/tags/目标检测/">目标检测</a><a href="/tags/视觉关系/">视觉关系</a><a href="/tags/深度相关网络/">深度相关网络</a></div><div class="post-nav"><a href="/2017/06/30/Deep-Forest-Towards-an-Alternative-to-Deep-Neural-Networks-阅读笔记/" class="next">Deep Forest: Towards an Alternative to Deep Neural Networks (阅读笔记)</a></div><div id="cloud-tie-wrapper" class="cloud-tie-wrapper"></div><script>var cloudTieConfig = {
  url: document.location.href,
  productKey: "true",
  target: "cloud-tie-wrapper"
};</script><script src="https://img1.cache.netease.com/f2e/tie/yun/sdk/loader.js"></script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://xuewenyuan.github.io"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Neural-Networks/">Neural Networks</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/文献阅读笔记/">文献阅读笔记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/CTC/" style="font-size: 15px;">CTC</a> <a href="/tags/深度森林/" style="font-size: 15px;">深度森林</a> <a href="/tags/深度学习/" style="font-size: 15px;">深度学习</a> <a href="/tags/目标检测/" style="font-size: 15px;">目标检测</a> <a href="/tags/视觉关系/" style="font-size: 15px;">视觉关系</a> <a href="/tags/深度相关网络/" style="font-size: 15px;">深度相关网络</a> <a href="/tags/Gabor-Filter/" style="font-size: 15px;">Gabor Filter</a> <a href="/tags/文本分割/" style="font-size: 15px;">文本分割</a> <a href="/tags/LSTM/" style="font-size: 15px;">LSTM</a> <a href="/tags/gcForest/" style="font-size: 15px;">gcForest</a> <a href="/tags/Deep-Learning/" style="font-size: 15px;">Deep Learning</a> <a href="/tags/Object-Detection/" style="font-size: 15px;">Object Detection</a> <a href="/tags/R-CNN/" style="font-size: 15px;">R-CNN</a> <a href="/tags/Neural-Networks/" style="font-size: 15px;">Neural Networks</a> <a href="/tags/Perceptron/" style="font-size: 15px;">Perceptron</a> <a href="/tags/Multilayer-Perceptron/" style="font-size: 15px;">Multilayer Perceptron</a> <a href="/tags/BackPropagation/" style="font-size: 15px;">BackPropagation</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/环境配置/" style="font-size: 15px;">环境配置</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/07/29/Detecting-Visual-Relationships-with-Deep-Relational-Networks-阅读笔记/">Detecting Visual Relationships with Deep Relational Networks（阅读笔记）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/30/Deep-Forest-Towards-an-Alternative-to-Deep-Neural-Networks-阅读笔记/">Deep Forest: Towards an Alternative to Deep Neural Networks (阅读笔记)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/21/DeepLearning4ObjectDetection/">Deep Learning for Object Detection</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/20/TexLive+Atom/">TexLive + Atom,教你配置炫酷的LaTex编译环境</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/08/19/Paragraph-text-segmentation-into-lines-with-Recurrent-Neural-Networks-阅读笔记/">Paragraph text segmentation into lines with Recurrent Neural  Networks（阅读笔记）</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/07/14/Neural-Networks-1/">Neural Networks(Part 1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/27/How-To-Understand-Gabor-Filter/">如何理解Gabor滤波器</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/24/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://yuhaomin.github.io/" title="俞浩敏" target="_blank">俞浩敏</a><ul></ul><a href="http://blog.csdn.net/jzwong" title="王建柱" target="_blank">王建柱</a><ul></ul><a href="http://blog.csdn.net/gyarenas" title="耿阳李敖" target="_blank">耿阳李敖</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2017 <a href="/." rel="nofollow">Wenyuan.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.0.47/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>